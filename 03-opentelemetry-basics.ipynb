{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenTelemetry Basics\n",
    "\n",
    "Now that we have an understand of what is happening in the Azure, we need to understand how we can get the data from our application to Azure. In this section we'll be using OpenTelemetry to collect telemetry data from our application and send it to Azure Application Insights.\n",
    "\n",
    "[OpenTelemetry](https://opentelemetry.io/) is an open-source observability framework that provides a set of APIs, libraries, agents, and instrumentation to enable observability for applications. It allows developers to collect and export telemetry data such as traces, metrics, and logs from their applications to various backends.\n",
    "\n",
    "\n",
    "Before we can start we need to install the OpenTelemetry SDK and get our connection string from Azure Application Insights. \n",
    "\n",
    "\n",
    "We can split OpenTelemetry into the following components:\n",
    "\n",
    "\n",
    "## SDK´s\n",
    "\n",
    "The OpenTelemetry SDK is the core component of OpenTelemetry that provides the APIs and libraries for collecting telemetry data. this is running as part of your application. The SDK is responsible for instrumenting your application code, collecting telemetry data, and exporting it to a backend.\n",
    "\n",
    "The SDK is available for various programming languages, including Java, Python, JavaScript, Go, and .NET. Each language has its own SDK implementation, but they all follow the same core principles and concepts of OpenTelemetry.\n",
    "\n",
    "We´ll be using opentelemetry with the Azure monitor backend as it has a built in exporter.\n",
    "\n",
    "### Enrichment & extension libraries\n",
    "\n",
    "Open telemetry has libraries for quite a few languages that allow to auto enrichment. \n",
    "\n",
    "In dotnet this could be teh OpenTelemetry.Extensions.Hosting, or opentelemetry-instrumentation-httpx in python. \n",
    "\n",
    "\n",
    "You can find the list of python enrichment libraries here [Python Enrichment Libraries](https://github.com/open-telemetry/opentelemetry-python-contrib/tree/main/instrumentation#readme).\n",
    "\n",
    "\n",
    "## Collectors\n",
    "\n",
    "The OpenTelemetry Collector is a vendor-agnostic proxy that can receive, process, and export telemetry data. It supports receiving telemetry data in multiple formats (for example, OTLP, Jaeger, Prometheus, as well as many commercial/proprietary tools) and sending data to one or more backends. It also supports processing and filtering telemetry data before it gets exported.\n",
    "\n",
    "You can read more here [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/).\n",
    "\n",
    "## Exporters\n",
    "Exporters are components that send telemetry data to a backend. OpenTelemetry provides a set of built-in exporters for various backends, including Azure Application Insights, Jaeger, Prometheus, and others. You can also create custom exporters to send data to other backends.\n",
    "\n",
    "You see read more here [Exporters](https://opentelemetry.io/docs/concepts/components/#exporters).\n",
    "\n",
    "## Standalone agent\n",
    "\n",
    "The OpenTelemetry Standalone Agent is a lightweight agent that can be deployed alongside your application to collect telemetry data. It can receive telemetry data from the OpenTelemetry SDK and export it to a backend. The Standalone Agent is useful for scenarios where you want to decouple the telemetry collection from your application code or when you want to use a different language for the agent than the one used in your application. This could also be in a scenario where you have network isolation and need to tunnel all telemetry data through a single host.\n",
    "\n",
    "You can read more here [OpenTelemetry Standalone Agent](https://opentelemetry.io/docs/collector/standalone-agent/).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Connecting OpenTelemetry to Azure Application Insights\n",
    "\n",
    "To connect our application to Azure Application Insights, we need to provide a connection string. This connection string contains the necessary information for our application to send telemetry data to the Application Insights resource in Azure. In a production environment, you would typically store this connection string in a secure location such as via a configuration management system.\n",
    "\n",
    "Let's fetch the connection string from Azure Application Insights. we can get the connection string from either the azure portal or the CLI\n",
    "\n",
    "<details>\n",
    "  <summary>Using the Azure portal</summary>\n",
    "\n",
    "1. Open the Azure portal and navigate to your Application Insights resource.\n",
    "2. In the overview blade, copy the \"Connection String\". \n",
    "</details>\n",
    "<details>\n",
    "  <summary>Using the Azure CLI</summary>\n",
    "\n",
    "```bash\n",
    "$UserInitials = \"ABCD\" # Replace with your initials\n",
    "$SystemName = \"msows${UserInitials}\" # Microsoft Observability Workshop - msows\n",
    "$appiName = \"appi-{$SystemName}-dev\"\n",
    "az resource show -n $appiName -g \"rg-obersvabilityworkshop-dev\" --resource-type \"microsoft.insights/components\" --query properties.ConnectionString \n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "The connection string will look like this:\n",
    "\n",
    "```bash\n",
    "InstrumentationKey=1234567890-1234-1234-1234-1234567890;IngestionEndpoint=https://swedencentral-0.in.applicationinsights.azure.com/;LiveEndpoint=https://swedencentral.livediagnostics.monitor.azure.com/;ApplicationId=1234567890-12345-12345-12345-12345-1234567890\n",
    "```\n",
    "\n",
    "\n",
    "For this workshop, we will be using an environment file This is a common practice for local development and testing.\n",
    "\n",
    "Create a file called `.env` in the root of your project directory. This file will contain the connection string for Application Insights. The format of the file should be as follows:\n",
    "\n",
    "```bash\n",
    "APPLICATIONINSIGHTS_CONNECTION_STRING=\"Paste connection string here\"\n",
    "```\n",
    "\n",
    "From here any application that uses the OpenTelemetry SDK will be able to read the connection string from this file via the load_dotenv() function. This is a common practice for local development and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your python environment\n",
    "\n",
    "Let's get a python environment up and running. We'll be using a virtual environment to isolate our dependencies. This is a common practice for python development.\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python3 -m venv .venv\n",
    "# Activate the virtual environment\n",
    "source .venv/bin/activate  \n",
    "# Install the required packages\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "In the jupyter notebook you can select which kernel to use in the upper right corner. Select the kernel that corresponds to the virtual environment you just created.\n",
    "\n",
    "\n",
    "#### Python code\n",
    "\n",
    "First lets setup the basic auto instrumentation for the OpenTelemetry SDK. This will  instrument your application to collect telemetry data without requiring major code changes. The OpenTelemetry SDK provides a set of APIs and libraries that can be used to instrument your application and collect telemetry data. We'll take a look them in a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from opentelemetry import trace\n",
    "import random\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "configure_azure_monitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have initialized the OpenTelemetry SDK, we can start collecting telemetry data. We'll start by using the logger. and then tracer functionality.\n",
    "\n",
    "The logger is the standard log implementation, it allows us to log messages at different levels (debug, info, warning, error, critical). The logger is a standard python logger and can be used in the same way as the standard python logger. The tracer is a bit different, it is used to create spans. A span is a single unit of work within trace, and a trace is a collection of spans that represent a end to end operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger = logging.getLogger()  # Logging telemetry will be collected from logging calls made with this logger and all of it's children loggers.\n",
    "tracer = trace.get_tracer(\"Jupyter_Notebook\") # Tracing telemetry will be collected from spans created with this tracer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some a log to check if everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"This is an info message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if the OpenTelemetry SDK is working correctly, you'll should be able to see the logs in Application Insights. You can do this by going to the Application Insights resource in the Azure portal and navigating to the Transaction search and you should be able to see your logs. It should look something like this: \n",
    "\n",
    "**Note** it might take a minute or two for the logs to show up in Application Insights.\n",
    "\n",
    "![Transaction Search](.files/appi_Transaction_search.png)\n",
    "\n",
    "You can also query for logs via the azCLI by running the following command in the Azure CLI:\n",
    "\n",
    "```bash\n",
    "az monitor app-insights query --analytics-query \"traces\" -g \"rg-obersvabilityworkshop-dev\" -a \"appi-CHANGE ME-dev\"\n",
    "```\n",
    "\n",
    "If we want to enrich logs with metadata we can also do this, In python this is called extra, and in azure it'll show up under Custom Properties to the logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logtypes = [logging.DEBUG,logging.INFO,logging.WARNING,logging.ERROR,logging.CRITICAL ]\n",
    "\n",
    "extra = {\n",
    "    \"workload\": \"workshop\",\n",
    "    \"component\": \"03-open-telemetry-basics\",\n",
    "    \"environment\": \"Development\", # You'd get this from the environment variables in a real application\n",
    "}\n",
    "\n",
    "for logtype in logtypes:\n",
    "    logger.log(logtype, f\"This is an {logtype} message\", extra=extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also enrich the logs further using enrichers, More can be found here about them-\n",
    "* https://opentelemetry.io/docs/collector/transforming-telemetry/#enriching-telemetry-with-resource-attributes\n",
    "* https://opentelemetry.io/docs/collector/transforming-telemetry/#adding-or-deleting-attributes\n",
    "\n",
    "We can also set custom properties for our the application using opentelemetry environment variables. here we would populate the service name and version, and normally we set these variables at deployment a ci/cd pipeline with the git commit hash, or the version of the application. This is a good practice for production applications. \n",
    "\n",
    "Try setting the following environment variables in our .env file and see how they show up in Application Insights. (You'll need restart the notebook kernel for the changes to take effect)\n",
    "\n",
    "```bash\n",
    "OTEL_RESOURCE_ATTRIBUTES=\"service.name=msows-application,service.version=1.0.0,service.instance.id=1234567890\"\n",
    "```\n",
    "\n",
    "you can see the entire list of environment variables that opentelemetry collects by itself [here](https://opentelemetry.io/docs/specs/otel/configuration/sdk-environment-variables/).\n",
    "\n",
    "They also allow to to set sample rates for the different telemetry types. This is useful if you want to limit the amount of data that is sent to Application Insights. For example, if you only want to send 10% of the traces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracers and Spans\n",
    "\n",
    "Now that we've set up the basic auto instrumentation, we can start monitoring specific functions or processes within our application. This can be done by using spans. they allow us to measure the duration, track the flow of execution, and capture metadata\n",
    "\n",
    "When using spans we have a few different options to enrich the logs, setting the status or marking events.\n",
    "\n",
    "```python\n",
    "with tracer.start_as_current_span(\"Query Example\") as span:\n",
    "    span.set_attribute(\"Query\", \"Select * from users where userId = %1\") # adding a custom dimension to the log entry\n",
    "    span.set_status(trace.StatusCode.OK) # setting the status to OK\n",
    "    span.add_event(\"query committed\", {\"Table\": \"users\"}) # logging a separate event in the span linked via the parent traceID\n",
    "```\n",
    "\n",
    "\n",
    "Here is a mock example of spans, Here we aren't really measuring anything but think about external apis, database calls or any other process that we want to measure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracer.start_as_current_span(\"Query loop Example\") as span:\n",
    "    for i in range(0,10):\n",
    "        with tracer.start_as_current_span(\"inner loop\") as span:\n",
    "            span.set_attribute(\"Query\", \"Select * from users where userId = %1\")\n",
    "            time.sleep(random.uniform(0.1, 0.01)) \n",
    "            with tracer.start_as_current_span(\"Query Execution\") as child:\n",
    "                try:\n",
    "                    child.set_attribute(\"Query\", \"Select * from users where userId = %1\")\n",
    "                    time.sleep(random.uniform(1, 0.1)) \n",
    "                    result = [1, 2, 3]  # Simulate a database query result\n",
    "                except Exception as e:\n",
    "                    time.sleep(random.uniform(0.2, 0.1))\n",
    "                    child.set_status(trace.StatusCode.ERROR, \"Query Failure\")\n",
    "                    child.record_exception(e)\n",
    "                child.set_status(trace.StatusCode.OK)\n",
    "                child.set_attribute(\"Results\",  len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to implement logging in the below example, it's just a small piece of code that inserts data into a database, and then makes a query to it.\n",
    "\n",
    "Try adding monitoring that does the follow:\n",
    "* How long does it take to insert the data into the database?\n",
    "* How long does it take to query the data from the database?\n",
    "* How long does it take to generate all the user data?\n",
    "* How many results are returned from the query?\n",
    "* The status of the query (success or failure)\n",
    "* Did any exceptions occur during the process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from faker import Faker\n",
    "import os\n",
    "\n",
    "class sql:\n",
    "    def ExecuteQuery(self, query, parameters = ()) -> None:\n",
    "        if not os.path.isfile(\"sqlite3.db\"):\n",
    "            self.SetupSQLiteDB()\n",
    "        con = sqlite3.connect(\"sqlite3.db\")\n",
    "        cur = con.cursor()\n",
    "        try:\n",
    "            resultset = cur.execute(query, parameters)\n",
    "            result = resultset.fetchall()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            con.close()\n",
    "        return result\n",
    "\n",
    "    def SetupSQLiteDB(self) -> None:\n",
    "        with self.tracer.start_as_current_span(\"SetupSQLiteDB\") as span:\n",
    "            con = sqlite3.connect(\"sqlite3.db\")\n",
    "            cur = con.cursor()\n",
    "            cur.execute(\n",
    "                \"CREATE TABLE users(unique id ,name, birthday, current_location, email, phone)\"\n",
    "            )\n",
    "            con.commit()\n",
    "            con.close()\n",
    "\n",
    "    def GenerateUserData(self, count=200) -> None:\n",
    "        if not os.path.isfile(\"sqlite3.db\"):\n",
    "            self.SetupSQLiteDB()\n",
    "        for i in range(100):\n",
    "            try:\n",
    "                fake = Faker()\n",
    "                id = i\n",
    "                name = fake.name()\n",
    "                birthday = fake.date_of_birth(minimum_age=18, maximum_age=90)\n",
    "                current_location = fake.city()\n",
    "                email = fake.email()\n",
    "                phone = fake.phone_number()\n",
    "                con = sqlite3.connect(\"sqlite3.db\")\n",
    "                cur = con.cursor()\n",
    "                cur.execute(\n",
    "                    \"INSERT INTO users VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "                    (id, name, birthday, current_location, email, phone),\n",
    "                )\n",
    "                con.commit()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            finally:\n",
    "                con.close()\n",
    "                    \n",
    "sql = sql()\n",
    "sql.ExecuteQuery(\"SELECT * FROM users where name like ?\", ('%adam%',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "  <summary>Solution idea</summary>\n",
    "\n",
    "```python\n",
    "import sqlite3\n",
    "from faker import Faker\n",
    "from opentelemetry import trace\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "class sql:\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "\n",
    "    def ExecuteQuery(self, query, parameters = ()) -> None:\n",
    "        with self.tracer.start_as_current_span(\"ExecuteQuery\") as span:\n",
    "            if not os.path.isfile(\"sqlite3.db\"):\n",
    "                self.SetupSQLiteDB()\n",
    "            span.set_attribute(\"Query\", query)\n",
    "            with self.tracer.start_as_current_span(\"Query Execution\") as child:\n",
    "                try:\n",
    "                    con = sqlite3.connect(\"sqlite3.db\")\n",
    "                    cur = con.cursor()\n",
    "                    resultset = cur.execute(query, parameters)\n",
    "                    result = resultset.fetchall()\n",
    "                except Exception as e:\n",
    "                    span.set_status(trace.StatusCode.ERROR, \"Query Failure\")\n",
    "                    span.record_exception(e)\n",
    "                    raise e\n",
    "                finally:\n",
    "                    con.close()\n",
    "                span.set_status(trace.StatusCode.OK)\n",
    "                span.add_event(\"Query results\", len(result))\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def SetupSQLiteDB(self) -> None:\n",
    "        with self.tracer.start_as_current_span(\"SetupSQLiteDB\") as span:\n",
    "            try:\n",
    "                con = sqlite3.connect(\"sqlite3.db\")\n",
    "                cur = con.cursor()\n",
    "                cur.execute(\n",
    "                    \"CREATE TABLE users(unique id ,name, birthday, current_location, email, phone)\"\n",
    "                )\n",
    "                con.commit()\n",
    "                span.add_event(\"committed\", {\"Table\": \"users\"})\n",
    "                span.set_status(trace.StatusCode.OK)\n",
    "            except Exception as e:\n",
    "                span.set_status(trace.StatusCode.ERROR, \"Error inserting user data\")\n",
    "                span.record_exception(e)\n",
    "            finally:\n",
    "                con.close()\n",
    "\n",
    "    def GenerateUserData(self, count=200) -> None:\n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"GenerateUserData\"\n",
    "        ) as span:\n",
    "            if not os.path.isfile(\"sqlite3.db\"):\n",
    "                self.SetupSQLiteDB()\n",
    "            span.set_attribute(\"count\", count)\n",
    "            for i in range(100):\n",
    "                with self.tracer.start_as_current_span(\"InsertUser\") as span:\n",
    "                    try:\n",
    "                        fake = Faker()\n",
    "                        id = i\n",
    "                        name = fake.name()\n",
    "                        birthday = fake.date_of_birth(minimum_age=18, maximum_age=90)\n",
    "                        current_location = fake.city()\n",
    "                        email = fake.email()\n",
    "                        phone = fake.phone_number()\n",
    "                        con = sqlite3.connect(\"sqlite3.db\")\n",
    "                        cur = con.cursor()\n",
    "                        cur.execute(\n",
    "                            \"INSERT INTO users VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "                            (id, name, birthday, current_location, email, phone),\n",
    "                        )\n",
    "                        con.commit()\n",
    "                        span.set_status(trace.StatusCode.OK)\n",
    "                    except Exception as e:\n",
    "                        span.set_status(trace.StatusCode.ERROR, \"Error inserting user data\")\n",
    "                        span.record_exception(e)\n",
    "                    finally:\n",
    "                        con.close()\n",
    "                    \n",
    "sql = sql()\n",
    "sql.ExecuteQuery(\"SELECT * FROM users where id = ?\", (1,))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we make external to other services these will also be auto instrumented. for example. if we connect to a storage account using the azure storage sdk.\n",
    "\n",
    "```python\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "with tracer.start_as_current_span(name=\"BlobServiceClient\")\n",
    "    client = BlobServiceClient.from_connection_string('connectionstring')\n",
    "    client.create_container('data')  # Call will be traced\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrumenting a FastAPI application\n",
    "\n",
    "In the 03 folder there is a simple FastAPI application that we can use to test the OpenTelemetry SDK. The application is a simple CRUD application that allows us to create, loan, and return books. it's a simple library system, It was 'vibe' coded in a few minutes using github copilot, The application uses SQLite as the database, the application is a simple REST API that allows us to interact with the database.\n",
    "\n",
    "The task is:\n",
    "\n",
    "* Add OpenTelemetry SDK to the application and instrument it to collect telemetry data.\n",
    "* Add logging to the application and send the logs to Application Insights.\n",
    "* Add spans when applicable to the application and send the information to Application Insights.\n",
    "* setup the opentelemetry-instrumentation-sqlite3 library to instrument the sqlite3.\n",
    "\n",
    "\n",
    "Look at the next section for examples on what to collect, and reflect if there's anything else you would also like to collect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different use/cases\n",
    "\n",
    "\n",
    "Alright now that we have a basic understanding of logging, and telemetry, let\\s think about a few different use-cases, and what we might want to monitor. This is a good exercise to do before we start implementing telemetry in our application. We want to make sure that we are collecting the right data, both for debugging, operations and for business purposes.\n",
    "\n",
    "\n",
    "Let's think about a few different workloads and use-cases.\n",
    "\n",
    "#### Web application\n",
    "\n",
    "<details>\n",
    "  <summary>Logs</summary>\n",
    "\n",
    "* Who is making requests to the application?\n",
    "* What requests are being made to the application?\n",
    "* What errors are being returned by the application?\n",
    "* What is the response time of the application?\n",
    "* What is the status code of the application?\n",
    "* What is the size of the response?\n",
    "* What is the user agent of the request?\n",
    "</details>\n",
    "<details>\n",
    "  <summary>Traces</summary>\n",
    "\n",
    "* How long did it take to process the request?\n",
    "* How long did it take to execute each function in the request?\n",
    "* How long did it take to execute each SQL query in the request?\n",
    "* How long did it take to execute each API call in the request?\n",
    "* How long did it take to execute each external service call in the request, and which dependencies do we have ?\n",
    "</details>\n",
    "<details>\n",
    "<summary>Metrics</summary>\n",
    "\n",
    "* How much RAM and CPU did we utilize?\n",
    "* How much IO did we use?\n",
    "* what is the number of requests per second?\n",
    "* What is the number of errors per second?\n",
    "  \n",
    "</details>\n",
    "\n",
    "#### Data Crawler\n",
    "\n",
    "<details>\n",
    "  <summary>Logs</summary>\n",
    "\n",
    "* How many records were crawled, and how long did it take to crawl them?\n",
    "* How many records were rejected, and why were they rejected?\n",
    "* How many records were loaded into the target system, and how long did it take to load them?\n",
    "* Did we have any errors or exceptions?\n",
    "* If we used an identity, what identity did each step use?\n",
    "* Are we being throttled, or rate limited by the target system(s)?\n",
    "</details>\n",
    "<details>\n",
    "  <summary>Traces</summary>\n",
    "\n",
    "* How long did it take to crawl the each record?\n",
    "* How are we spending the time, eg get next record, fetch payload, parse payload, process it, and store it?\n",
    "</details>\n",
    "<details>\n",
    "  <summary>Metrics</summary>\n",
    "\n",
    "* How much RAM and CPU did we utilize?\n",
    "* How much IO did we use?\n",
    "* How many records were crawled per second?\n",
    "* How many records were rejected per second?\n",
    "* How many records were loaded into the target system per second?\n",
    "\n",
    "\n",
    "  **note** Some of these metrics can maybe be used to calculated using the logs, but that only gives us on average and not if we have spikes, and drops\n",
    "</details>\n",
    "\n",
    "#### Data processor\n",
    "\n",
    "<details>\n",
    "  <summary>Logs</summary>\n",
    "\n",
    "  * What records were processed, and how long did it take to process them?\n",
    "  * How many records were rejected, and why were they rejected?\n",
    "  * Meta data about the records, eg size, type, format, etc.\n",
    "</details>\n",
    "<details>\n",
    "  <summary>Traces</summary>\n",
    "\n",
    "  * How long did it take to process each record?\n",
    "  * How are we spending the time, Waiting for openai, local cpu/gpu processing.\n",
    "</details>\n",
    "<details>\n",
    "  <summary>Metrics</summary>\n",
    "\n",
    "  * How much RAM and CPU did we utilize?\n",
    "  * How many records were processed per second?\n",
    "  * Whats is our IO usage?\n",
    "</details>\n",
    "\n",
    "#### ETL pipeline\n",
    "<details>\n",
    "  <summary>Logs</summary>\n",
    "\n",
    "* What pipeline is it?\n",
    "* Where is it running?\n",
    "* What started the pipeline? Manually, Scheduler, or a trigger? and if a trigger, what trigger?\n",
    "* If we used an identity, what identity did each step use?\n",
    "* How many records were processed, and how long did it take to process them?\n",
    "* How many records were rejected, and why were they rejected?\n",
    "* How many records were loaded into the target system, and how long did it take to load them?\n",
    "* Did we have any errors or exceptions\n",
    "  \n",
    "</details>\n",
    "<details>\n",
    "  <summary>Traces</summary>\n",
    "   \n",
    "* Data extraction, How long did it take to extract the data from the source system(s), and how how did we spend the time?\n",
    "* Data transformation, How long did it take to transform the data? how are we spending the time? eg what functions were called, and how long did they take?\n",
    "* Data loading. How long did it take to load the data into the target system?\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "  <summary>Metrics</summary>\n",
    "\n",
    "* How much RAM and CPU did we utilize?\n",
    "* How much IO did we use?\n",
    "* How many records were processed per second?\n",
    "* How many records were rejected per second?\n",
    "* How many records were loaded into the target system per second? \n",
    "\n",
    "**note** Some of these metrics can maybe be used to calculated using the logs, but that only gives us on average and not if we have spikes, and drops\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps.\n",
    "\n",
    "Let's take a look at a sample application and instrument it with open telemetry, and add some logging and tracing to it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
